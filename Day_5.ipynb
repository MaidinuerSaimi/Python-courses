{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaidinuerSaimi/Python-courses/blob/main/Day_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcHVX5qhsbVA"
      },
      "source": [
        "# Workshop 1.5 -- data exploration with Matplotlib\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The topic of today's workshop is **data exploration**.  This important topic tends to be an under-appreciated part of\n",
        "data science.  Although there are technical skills involves (mainly, slicing and dicing your Pandas dataframe,\n",
        "and knowing your way around the plotting functions), it is more accurately described as between an art and a science -- and it crucially involves your knowledge of the problem at hand.  You are looking out for *unexpected* features in your data, due to unforeseen issues in data collection, selection of your subjects, errors, data handling or conversion problems - or perhaps even unexpected but real phenomena in your data.\n",
        "\n",
        "### Expect the unexpected!\n",
        "Because you're looking for the unexpected, there is no fool-proof recipe for doing this.  However, in general it is a good idea to spend time *visualising* your data.  Doing so increases your chances of identifying issues, which allows you to clean up your data, and you will also learn about the process you're actually interested in, which will enable you to make better choices when you're building your model.\n",
        "\n",
        "### Why do this?\n",
        "A few bad apples spoil the bunch.  Outliers can have a major influence on the final model.  If your data has missing values, and you don't realize it and deal with it, your predictions can become unreliable.  If some columns have very skewed distributions, your model may end up all but ignoring these columns. In practice, time spent on data cleaning and pre-processing brings more benefits than time spent on the modeling itself.\n",
        "\n",
        "### Appearances matter\n",
        "To properly explore the data it will be necessary to fine-tune your plots, because different settings (including ones you may think of as purely aesthetic, such as color schemes) can make a substantial difference in how informative the plot is -- but equally it is possible for artefacts to dominate the plot, or to emphasize an unimportant aspect.  Again, this is an art, and practice makes perfect - don't expect a recipe, but things in this workshop will get you started.\n",
        "\n",
        "### Real data, real issues\n",
        "We are going to focus on a real data set of over 40,000 physical measurements of aortic and pulmonary heart valve diameters of healthy donors, made by a North American company.  Other characteristics of the donors (sex, height, weight, age) are also available.  This allows us to build a model predicting these heart valve diameters from their donor's height etc., which is helpful to predict the required diameter when someone needs a heart valve replacement.\n",
        "\n",
        "(The data is anonymized and slightly modified, so don't use the models you build today if you ever need a new valve)\n",
        "\n",
        "## Workshop\n",
        "\n",
        "Run the appropriate code below to load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9mhM1WCyUPb"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVNIMalxyb10"
      },
      "outputs": [],
      "source": [
        "## If you're running this on Google Colab, use this to upload your file:\n",
        "\n",
        "from google.colab import files\n",
        "files.upload();  ## Upload 'Heart valve dissection data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_ER6tnmyn6j"
      },
      "outputs": [],
      "source": [
        "## Load your data.\n",
        "## (If you're running Jupyter and the data is not in the directory where you started Jupyter, modify the path below)\n",
        "\n",
        "df = pd.read_csv(\"Heart valve dissection data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaiteVxIsbVF"
      },
      "source": [
        "Use `df.info()` and `df.describe()` to get a first sense of the data.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "    \n",
        "## Question 1\n",
        "    \n",
        "- 1) What are the target variable(s), what are predictive variables?\n",
        "- 2) What do the columns mean?  What are their units? Which columns should we ignore when building a model?\n",
        "- 3) Is there any missingness?\n",
        "- 4) Do any variables require pre-processing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxySkmOtsbVG"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGrpqzQfsbVG"
      },
      "source": [
        "Use `df.drop` to drop the first (unnamed) index column, and the \"Id\" column.  Then remove all rows with missing AVD values - we want to predict the aortic valve diameter, so imputation is not an option.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "    \n",
        "## Question 2\n",
        "    \n",
        "How many rows does the dataframe have before and after removal of records with missing AVD values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDILo43ZsbVG"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETZKm1hX0wUK"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzxy1FjksbVH"
      },
      "source": [
        "Convert the values in the column \"dissection_date\" to Panda's `DateTime` format, using the `pd.to_datetime` function.\n",
        "\n",
        "Then use `sns.histplot` to plot histograms of the data in the five columns \"Age\", \"Height_cm\", \"Weight_kg\", \"dissection_date\" and \"AVD\".  Use `plt.show()` after each plot, otherwise you will only see the last one.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "    \n",
        "## Question 3\n",
        "    \n",
        "Do these histograms show up any obvious issues?  What is the reason for the jagged appearance and/or white gaps in some plots?  (Is it the plotting, or is this a feature of the data?)  Try plotting again, but now setting one of `bins=100`, `discrete=True`, `binwidth=2`.  Can you explain why the plots look different now?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w41aPxfvyxtq"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzBReuiisbVH"
      },
      "source": [
        "Make a separate histogram of height, using only donors with a height between 50 and 150 cm.  Use `binwidth=1`.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "    \n",
        "## Question 4\n",
        "    \n",
        "What do you think is the explanation for the peaks at ~61cm, ~92cm and ~122cm?  Could this be a problem when we build a model usign these data?  What could you do to mitigate this issue?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kr1V0f7sbVH"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsmkNRoCsbVH"
      },
      "source": [
        "A powerful way to visualise your data is using scatterplots -- these plots can convey a lot of information, and\n",
        "show how to variables are related.  Look at the manual page and tutorials in Matplotlib for the many options and\n",
        "different plots you can make.\n",
        "\n",
        "Let us start with `sns.scatterplot` of \"Age\" and \"Height_cm\", with default parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZadV4nFLsbVH"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTYE5qvFsbVH"
      },
      "source": [
        "Notice that the dataset is too large for this standard plot; you get no sense of where the highest density is,\n",
        "because points are plotted over one another.  A simple first way to deal with this is decreasing the point size,\n",
        "with parameter `s=2`.\n",
        "\n",
        "In addition the \"Age\" variable is discrete, so all points lie on vertical lines.  In fact also \"Height_cm\" is\n",
        "(mostly) discretized.  Jittering is the standard way to address this -- adding random noise.\n",
        "\n",
        "Do this by using `df[\"Age\"] + np.random.normal(0, 0.3, len(df))` as your `x` parameter, and similar for the \"Height_cm\" variable for `y`.\n",
        "The `np.random.normal` function generates `len(df)` normally-distributed numbers with mean 0 and\n",
        "standard deviation 0.3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ibm57X2sbVH"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J__6UybVsbVH"
      },
      "source": [
        "This starts to show some features -- you can see the heights rounded to nearest feet\n",
        "that you identified using the histogram.  Still, there seems to be overplotting going on.  You can make\n",
        "the points transparent to get a better sense of the density.  Make the same plot but now adding the keyword\n",
        "`alpha=0.2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh_3P_U-sbVI"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt2TACwGsbVI"
      },
      "source": [
        "Instead of a scatterplot, you can achieve a similar overview using a 2D histogram.  This gives you more\n",
        "flexibility for very large data sets.  Start by making a `sns.histplot` for the same variables, and defaults\n",
        "for all keyword parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMZ1-uBusbVI"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUOA24GIsbVI"
      },
      "source": [
        "It's recognizably the same shape, but the default bin size for height is far too small.\n",
        "Run again with keywords `bins=(50,50)`, and also include `cbar=True` to add a legend so you can\n",
        "relate the color to the actual frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESGgA--EsbVI"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTMgDFu0sbVI"
      },
      "source": [
        "That's much better, but it's also obvious that there is a huge density at the bottom left, and the plot\n",
        "is otherwise dominated by much lower counts that are not well separated visually.  To see more detail in\n",
        "both low and high frequency regions, we can do a log transform on the counts.  In the `histplot` this\n",
        "is called \"normalization\" (don't ask me), so add the keyword `norm=mpl.colors.LogNorm()`.\n",
        "                           \n",
        "(Seaborn provides default values for `vmin` and `vmax`, but these don't play well with LogNorm,\n",
        " so also add `vmin=None, vmax=None` to override these defaults, otherwise you get errors.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzmGoyI7sbVI"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4s1u_L9sbVI"
      },
      "source": [
        "That's a lot better already.  Still the default color scheme isn't great.  Also, some vertical stripes are visible,\n",
        "which is caused by discretization of \"Age\" and bins sometimes catching different number of age categories.\n",
        "Instead we can set the bin width directly.\n",
        "\n",
        "Add the keywords `binwidth=(1,3)` (and remove `bins=(50,50)`), and also add `cmap=\"RdYlGn_r\"` to choose a reversed\n",
        "Red-Yellow-Green color map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QJN2gz9sbVI"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CukqK0bnsbVI"
      },
      "source": [
        "Now, make the same plot, but include female donors only.  Do this by replacing `df` by `df[df[\"Sex\"] == \"female\"]`\n",
        "in the invocation of `histplot`\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "    \n",
        "## Question 5\n",
        "    \n",
        "Do you see anything strange in ths plot? Look at the height distribution.  Can you give an argument whether or not this is likely to be an artefact, or real?\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGKxiuY5sbVI"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lacll0ENsbVI"
      },
      "source": [
        "It is tedious to make these plots for all pairs of variables.  Seaborn has a handy function, `pairplot` to do this for\n",
        "you.  It has three main parameters, `data` for the dataframe, `vars` a list of variables, and `kind` to select the kind of\n",
        "plot you need.\n",
        "\n",
        "Run `sns.pairplot` with `vars=[\"Age\",\"Height_cm\",\"Weight_kg\",\"AVD\"]` and `kind=\"hist\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-7YFgalsbVI"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZvLrsl9sbVI"
      },
      "source": [
        "That looks similar to our initial plot before our tweaks.  We can tweak `pairplot` in the same way\n",
        "by giving it keywords, through the `plot_kws` parameter which expects a dictionary of keywords and values.\n",
        "\n",
        "Run the command above again but now adding `plot_kws={\"bins\":50,\"cmap\":\"RdYlGn_r\",\"vmin\":None,\"vmax\":None,\"norm\": mpl.colors.LogNorm()}`\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "    \n",
        "## Question 6\n",
        "\n",
        "Can you recognize the two issues we identified before?  Do you see any more potential issues?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE7lG0ArsbVI"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARWH1nPKsbVJ"
      },
      "source": [
        "## Batch effects\n",
        "\n",
        "Batch effects are common issues with experimental data.  They occur because the conditions in which data are collected\n",
        "change over time, because of slight differences in the protocol used by different experimentalists or at\n",
        "different locations (e.g. different hospitals each collecting part of the data).\n",
        "\n",
        "Here all we know is that the data are collected by a single company, but over several years.  Let's plot some variables\n",
        "against this dissection time to ensure no systematic problems occured in some time periods.\n",
        "\n",
        "Use a `sns.histplot` to plot \"AVD\" against \"dissection_date\".  Use parameters\n",
        "`bins=(50,50), cbar=True, norm=mpl.colors.LogNorm(), vmin=None, vmax=None, cmap=\"RdYlGn_r\"`.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "    \n",
        "## Question 7\n",
        "    \n",
        "Do you see any unexpected features?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmj_w9NWsbVJ"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o2DBAC4sbVJ"
      },
      "source": [
        "## Residual plots\n",
        "\n",
        "Residual plots are a powerful way to identify issues in your data - and potential model misfits.  \n",
        "\n",
        "The residual is the difference between a model prediction and the true value.  (The concept makes sense only\n",
        "for regression, as the \"difference\" between two classes is meaningless.)\n",
        "These residuals should be a small as possible.  If there is any systematic relation between the residuals and\n",
        "the explanatory variables, or between residuals and variables that should **not** matter\n",
        "(such as the day of the week), this is an indication of a problem.\n",
        "\n",
        "Residuals are also helpful for identifying outliers -- points where the \"true\" and predicted values are very different.  These may be mistakes in your data (either outcome or preditors).\n",
        "\n",
        "To make a residual plot, you need a model.  For the purpose of data exploration, let's keep it simple and\n",
        "fit a `sklearn.linear_model.LinearRegression` model to the data (variables height, weight and age, target \"AVD\").\n",
        "After running `fit`, run `predict` on the training data (again, for our current purpose we do not worry about\n",
        "overtraining), and compute the difference of the target and the prediction.  Make this a new column named \"residual\" in your dataframe (`df['residual'] = ...`).\n",
        "\n",
        "Then, make a `histplot` of \"residual\" against \"dissection_date\", using the color scheme as above, and `bins=(100,25)`.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "    \n",
        "## Question 8\n",
        "    \n",
        "Is this the same issue you saw in question 7?  Will this be an issue for building a model using these data?\n",
        "Can you suggest some strategies to mitigate this issue?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I64i7lsd0TYJ"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "generative_ai_disabled": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}